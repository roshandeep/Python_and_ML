{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pos</td>\n",
       "      <td>I loved this movie and will watch it again. Or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pos</td>\n",
       "      <td>A warm, touching movie that has a fantasy-like...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pos</td>\n",
       "      <td>I was not expecting the powerful filmmaking ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neg</td>\n",
       "      <td>This so-called \"documentary\" tries to tell tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pos</td>\n",
       "      <td>This show has been my escape from reality for ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                             review\n",
       "0   pos  I loved this movie and will watch it again. Or...\n",
       "1   pos  A warm, touching movie that has a fantasy-like...\n",
       "2   pos  I was not expecting the powerful filmmaking ex...\n",
       "3   neg  This so-called \"documentary\" tries to tell tha...\n",
       "4   pos  This show has been my escape from reality for ..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('moviereviews2.tsv', sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> I loved this movie and will watch it again. Original twist to Plot of Man vs Man vs Self. I think this is Kurt Russell's best movie. His eyes conveyed more than most actors words. Perhaps there's hope for Mankind in spite of Government Intervention?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "display(Markdown('> '+df['review'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label      0\n",
       "review    20\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5980"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(inplace=True)\n",
    "\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 blanks:  []\n"
     ]
    }
   ],
   "source": [
    "blanks = []  # start with an empty list\n",
    "\n",
    "for i,lb,rv in df.itertuples():  # iterate over the DataFrame\n",
    "    if type(rv)==str:            # avoid NaN values\n",
    "        if rv.isspace():         # test 'review' for whitespace\n",
    "            blanks.append(i)     # add matching index numbers to the list\n",
    "        \n",
    "print(len(blanks), 'blanks: ', blanks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5980"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(blanks, inplace=True)\n",
    "\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neg    2990\n",
       "pos    2990\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['review']\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Na√Øve Bayes:\n",
    "text_clf_nb = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "# Linear SVC:\n",
    "text_clf_lsvc = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('clf', LinearSVC()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidf',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, use_idf=True,\n",
       "                                 vocabulary=None)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf_nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = text_clf_nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[940  51]\n",
      " [136 847]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.87      0.95      0.91       991\n",
      "         pos       0.94      0.86      0.90       983\n",
      "\n",
      "    accuracy                           0.91      1974\n",
      "   macro avg       0.91      0.91      0.91      1974\n",
      "weighted avg       0.91      0.91      0.91      1974\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.confusion_matrix(y_test,predictions))\n",
    "print(metrics.classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidf',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, use_idf=True,\n",
       "                                 vocabulary=None)),\n",
       "                ('clf',\n",
       "                 LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                           fit_intercept=True, intercept_scaling=1,\n",
       "                           loss='squared_hinge', max_iter=1000,\n",
       "                           multi_class='ovr', penalty='l2', random_state=None,\n",
       "                           tol=0.0001, verbose=0))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf_lsvc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = text_clf_lsvc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[900  91]\n",
      " [ 63 920]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.93      0.91      0.92       991\n",
      "         pos       0.91      0.94      0.92       983\n",
      "\n",
      "    accuracy                           0.92      1974\n",
      "   macro avg       0.92      0.92      0.92      1974\n",
      "weighted avg       0.92      0.92      0.92      1974\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.confusion_matrix(y_test,predictions))\n",
    "print(metrics.classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_md\n",
    "import en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.6909e-01,  2.5307e-01, -5.7593e-01,  1.6235e-01,  1.6094e-01,\n",
       "       -1.9802e-01, -2.8971e-02, -2.5352e-01, -7.4811e-02,  2.1331e+00,\n",
       "       -4.6460e-01,  5.6153e-02, -5.0651e-01, -4.4885e-01, -4.7379e-01,\n",
       "        4.4561e-01, -1.2656e-01,  6.4420e-01,  1.6048e-01,  3.4406e-01,\n",
       "        1.1782e-01, -5.4542e-01, -3.9005e-01, -1.7133e-01, -1.0692e-01,\n",
       "        4.0355e-01,  1.4711e-01, -2.1785e-01,  2.3270e-01,  4.2617e-03,\n",
       "       -1.8534e-01,  6.4222e-02,  2.8307e-01, -4.6408e-03,  6.7360e-01,\n",
       "       -1.2961e-01,  3.6804e-02, -2.9297e-01, -1.0704e-01, -1.0144e-01,\n",
       "        3.6363e-01, -3.4643e-01,  8.9158e-02, -3.3299e-01, -1.4088e-01,\n",
       "       -1.1175e-03, -1.5586e-01, -9.2016e-02,  2.0663e-01, -7.1369e-01,\n",
       "       -4.2443e-01, -2.0274e-01, -4.9529e-01, -1.4574e-01, -1.2492e-01,\n",
       "        4.8865e-01,  9.1048e-02, -4.8057e-02,  2.1768e-01,  1.3217e-02,\n",
       "       -1.2011e-01, -3.8334e-01, -7.9559e-02, -2.4453e-01, -2.0607e-01,\n",
       "       -1.6878e-01, -1.3342e-01,  4.5845e-01,  4.1422e-02,  2.0499e-01,\n",
       "       -3.3955e-01,  1.7824e-01, -3.5252e-01,  1.0304e-01,  3.5595e-01,\n",
       "        1.9855e-01, -4.5609e-01, -5.1780e-02,  5.5408e-02, -5.9262e-02,\n",
       "        3.0413e-01,  1.2991e-01, -2.5488e-01,  3.6270e-01, -4.3209e-02,\n",
       "        2.3285e-01,  1.2294e+00,  1.1799e-01,  4.6054e-03,  7.7812e-03,\n",
       "       -2.1193e-01,  2.4265e-01, -2.0021e-01, -5.6427e-01, -2.8220e-01,\n",
       "       -4.6040e-01, -7.9735e-02,  3.3760e-02, -1.4892e-01,  1.4840e-01,\n",
       "        5.4642e-01,  5.7023e-01,  5.5902e-02, -1.3657e-01,  2.3033e-01,\n",
       "        2.2807e-01, -6.4512e-01, -1.0182e-01,  6.8341e-02, -3.1790e-01,\n",
       "        1.2691e-01, -1.1262e-02, -5.7404e-01, -2.8038e-01, -1.4787e-01,\n",
       "       -1.1296e-01,  3.8612e-01,  2.6175e-01, -9.5176e-02, -7.1672e-02,\n",
       "       -4.2501e-02, -3.2317e-01,  1.7194e-01, -3.0685e-01,  8.1819e-02,\n",
       "        5.6033e-01, -7.8356e-02, -1.7296e-01, -1.3717e-01,  1.2221e-01,\n",
       "        1.8733e-01, -1.5872e-01, -3.1311e-01,  1.3497e-01,  5.9967e-01,\n",
       "       -3.6567e-01, -3.4220e-01,  3.8889e-01,  8.4099e-02,  2.3086e-01,\n",
       "       -3.6435e+00, -2.1372e-01,  1.4358e-02, -2.8300e-01, -3.1654e-01,\n",
       "       -3.0101e-01, -1.6488e-01,  3.5519e-02, -2.9371e-01, -1.0814e-01,\n",
       "       -5.0092e-01,  2.9186e-01, -2.4973e-01, -1.3276e-01,  7.3195e-02,\n",
       "       -5.5337e-02, -5.0897e-02, -2.6640e-01, -9.7223e-02, -3.5488e-01,\n",
       "       -1.9095e-01, -1.3013e-01,  1.1006e-01,  8.3209e-02, -3.1144e-01,\n",
       "       -4.3890e-01,  1.7464e-01, -5.2185e-02,  2.4667e-01,  4.9340e-01,\n",
       "       -1.6702e-01, -1.3543e-01,  4.0953e-01, -3.5136e-01, -3.6243e-02,\n",
       "       -2.2589e-01, -8.6225e-01,  4.0991e-01, -6.4399e-01,  3.9345e-01,\n",
       "       -3.5752e-01,  3.4495e-01,  1.5168e-01, -3.0971e-01, -9.2843e-02,\n",
       "        4.2582e-01, -4.6271e-01,  4.8269e-01,  1.2593e-01, -2.5813e-01,\n",
       "       -2.2849e-01, -1.8986e-01, -1.4170e-01,  2.1845e-02,  7.7441e-02,\n",
       "       -3.2888e-01,  4.6039e-03, -2.5013e-02, -1.5104e-01, -7.6992e-03,\n",
       "        1.5680e-01,  2.6785e-01, -3.2104e-01, -1.7154e-01, -4.4894e-01,\n",
       "        1.5063e-01,  6.8067e-03,  1.4818e-01,  1.4068e-01,  1.2737e-01,\n",
       "       -7.1748e-01, -2.0078e-01, -3.7893e-01, -2.6166e-01,  1.7780e-01,\n",
       "       -4.2052e-02, -5.1617e-01, -2.0675e-01,  1.9628e-01, -3.2059e-01,\n",
       "       -1.6230e-01, -3.4745e-01,  3.5163e-02, -6.9429e-02, -2.5188e-01,\n",
       "       -1.8092e-02,  4.0608e-01,  1.4913e-01,  1.5859e-01, -4.4722e-02,\n",
       "       -3.0193e-01,  1.1825e-02,  1.5602e-01, -1.5593e-02, -9.6849e-02,\n",
       "       -2.2080e-01, -4.1823e-02,  4.5651e-01,  1.4732e-02, -1.8996e-01,\n",
       "       -6.4075e-01,  2.8110e-01,  2.9593e-01, -2.4946e-01,  3.4825e-01,\n",
       "       -2.9148e-01, -6.1145e-01,  3.8674e-02,  3.0281e-01,  2.1332e-01,\n",
       "       -1.5786e-01,  4.1419e-01,  4.6804e-01,  7.4988e-02,  4.0826e-02,\n",
       "        1.9257e-01, -1.6069e-01, -2.7996e-01, -7.5286e-02, -5.6603e-02,\n",
       "       -1.7315e-01,  6.0683e-01, -5.1229e-01, -1.3542e-01,  6.5854e-02,\n",
       "        3.3613e-01, -1.8186e-02,  3.9413e-01, -7.4138e-01, -3.5554e-02,\n",
       "       -1.3326e+00,  1.1044e-01, -6.0748e-01,  1.9463e-01,  2.9874e-02,\n",
       "       -3.3861e-02, -4.0727e-02,  1.1920e-01,  3.7909e-01,  4.3656e-01,\n",
       "       -9.9922e-02,  3.6707e-01, -1.3175e-02, -2.7368e-01, -1.7171e-01,\n",
       "        2.9368e-01, -2.3312e-02,  2.5316e-01, -6.8967e-02, -4.0257e-01,\n",
       "        9.1683e-02, -1.0386e-01,  4.1797e-01, -2.3610e-01, -3.3200e-02,\n",
       "       -1.8152e-01, -4.1914e-01, -2.3858e-01,  1.5172e-01,  1.3429e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(u'girl').vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lion lion 1.0\n",
      "lion cat 0.52654374\n",
      "lion pet 0.39923766\n",
      "cat lion 0.52654374\n",
      "cat cat 1.0\n",
      "cat pet 0.7505456\n",
      "pet lion 0.39923766\n",
      "pet cat 0.7505456\n",
      "pet pet 1.0\n"
     ]
    }
   ],
   "source": [
    "# Create a three-token Doc object:\n",
    "tokens = nlp(u'lion cat pet')\n",
    "\n",
    "# Iterate through token combinations:\n",
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(684831, 300)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nlp(u'love wife Simran')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love True 6.04035 False\n",
      "wife True 6.672992 False\n",
      "Simran True 7.3687935 False\n"
     ]
    }
   ],
   "source": [
    "for token in tokens:\n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "cosine_similarity = lambda vec1, vec2: 1 - spatial.distance.cosine(vec1, vec2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "king = nlp.vocab['king'].vector\n",
    "man = nlp.vocab['man'].vector\n",
    "woman = nlp.vocab['woman'].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we find the closest vector in the vocabulary to the result of \"man\" - \"woman\" + \"queen\"\n",
    "new_vector = king - man + woman\n",
    "computed_similarities = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in nlp.vocab:\n",
    "    # Ignore words without vectors and mixed-case words:\n",
    "    if word.has_vector:\n",
    "        if word.is_lower:\n",
    "            if word.is_alpha:\n",
    "                similarity = cosine_similarity(new_vector, word.vector)\n",
    "                computed_similarities.append((word, similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "computed_similarities = sorted(computed_similarities, key=lambda item: -item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['king', 'queen', 'commoner', 'highness', 'prince', 'sultan', 'maharajas', 'princes', 'kumbia', 'kings']\n"
     ]
    }
   ],
   "source": [
    "print([w[0].text for w in computed_similarities[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
